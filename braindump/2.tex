```latex
\subsubsection*{Vision: Was ist die Vision hinter dem Thema? Wie ordnet sich diese zu anderen Themen und grundsätzlichen Fragen ein?}

Die grundlegende Vision hinter der Thematik ist die \textbf{Transformation der Softwareentwicklung und Cybersicherheit durch die nahtlose Integration von Large Language Models (LLMs) mit traditionellen statischen Code-Analyse-Tools (SAST/SCA)}. Diese Synergie soll die \textbf{Qualität, Zuverlässigkeit und Sicherheit von Software erheblich verbessern}.

Dies soll erreicht werden, indem:
\begin{itemize}
    \item \textbf{Automatisierte Erkennung und Behebung von Code-Problemen (Bugs, Schwachstellen, Code Smells)} ermöglicht wird, die herkömmliche, regelbasierte Methoden oft übersehen oder nur unzureichend handhaben können.
    \item Die \textbf{Skalierbarkeit und Anwendbarkeit von Schwachstellen-Erkennungstechniken deutlich erhöht} wird, um mit der wachsenden Größe und Komplexität moderner Softwaresysteme Schritt zu halten.
    \item Der \textbf{manuelle Aufwand und der Ressourcenverbrauch reduziert} werden, insbesondere bei der zeitintensiven Überprüfung von SAST-Berichten und der Behebung von False Positives (FPs).
    \item \textbf{Intelligentere und adaptivere Systeme} geschaffen werden, die aus vergangenen Daten lernen und ihre Detektions- sowie Behebungsfähigkeiten kontinuierlich verbessern können.
    \item Eine \textbf{komplementäre Sicherheitsanalyse} entsteht, die die Stärken der präzisen, aber limitierten traditionellen Methoden mit den nuancierten, kontextsensitiven Verständnisfähigkeiten von LLMs verbindet.
\end{itemize}

Die \textbf{grundsätzliche Frage}, die hier adressiert wird, ist, wie man die \textbf{inherenten Herausforderungen der modernen Softwareentwicklung} – wie die rasante Evolution von Cyberbedrohungen, die Komplexität von Codebasen und die Ineffizienz manueller oder unflexibler Sicherheitstests – effektiv bewältigen kann. LLMs bieten hier eine \textbf{vielversprechende neue Richtung}, da sie eine menschenähnliche Fähigkeit zum Verstehen, Generieren und sogar Debuggen von Code mitbringen, die über die formalen Grenzen traditioneller Programmanalyse hinausgeht.
Das Thema ordnet sich in die umfassendere Bewegung der \textbf{Automatisierung von Software-Entwicklungsprozessen (DevOps, DevSecOps)} und der \textbf{KI-gestützten Cybersicherheit} ein, um Software robuster, sicherer und effizienter zu gestalten.

\subsubsection*{Potenziale und Ziele: Was ist das Potenzial/Bedeutung dieser Vision für zukünftige Methoden des Software Engineering? Was sind die Ziele und Metriken?}

Das Potenzial dieser Vision für zukünftige Methoden des Software Engineering ist weitreichend und umfasst:

\begin{itemize}
    \item \textbf{Erhebliche Steigerung der Code-Qualität und -Sicherheit:} LLMs können komplexere und kontextsensitivere Probleme (Bugs, Schwachstellen, Code Smells) identifizieren, die traditionelle SAST-Tools oft übersehen. Ihre Fähigkeit, tiefergehende logische Fehler zu erkennen, ergänzt die regelbasierten Ansätze ideal.
    \item \textbf{Automatisierung zeit- und ressourcenintensiver Prozesse:} Dies betrifft insbesondere die automatisierte Code-Revision und die \textbf{drastische Reduzierung von False Positives (FPs)} in Sicherheitsberichten. Dies führt zu einer \textbf{signifikanten Zeit- und Kosteneinsparung} im gesamten Softwareentwicklungszyklus.
    \item \textbf{Verbesserte Effizienz und Präzision der Analyse:} LLMs können spezifische Analyseteile korrekt durchführen, wie das Identifizieren von \textbf{Quellen (sources), Senken (sinks) und Sanitizern} innerhalb des Codes. Sie bieten zudem \textbf{intuitive Erklärungen in natürlicher Sprache} für ihre Vorhersagen, was das Debugging erheblich erleichtert und Entwicklern tiefere Einblicke ermöglicht.
    \item \textbf{Entwicklung intelligenter und adaptiver Systeme:} Durch die Integration mit Retrieval-Augmented Generation (RAG) können LLMs dynamisch auf aktuelle externe Informationen zugreifen und ihre Fähigkeiten kontinuierlich an neue Bedrohungslandschaften anpassen und verbessern.
    \item \textbf{Minderung von Datenschutz- und Sicherheitsrisiken:} Die Nutzung lokal hostbarer oder quelloffener LLMs (z.B. Llama-3-70B) ermöglicht es, sensible Code-Daten innerhalb der Infrastruktur zu verarbeiten, ohne sie an Dritte zu senden, wodurch Datenschutzbedenken adressiert werden.
\end{itemize}

\textbf{Die wichtigsten Ziele und Metriken für diesen Ansatz sind:}

\begin{itemize}
    \item \textbf{Effizienz der Fehlerbehebung:}
    \begin{itemize}
        \item \textbf{Signifikante Reduzierung der Gesamtzahl von Code-Problemen:} Zum Beispiel konnte in einer Studie die Zahl der Probleme in einem Projekt von 7.599 auf 1.058 gesenkt werden.
        \item \textbf{Erfolgsraten bei der Problembehebung:} Erzielung hoher Erfolgsraten bei der Behebung spezifischer Problemtypen, wobei \textbf{100\% Erfolgsrate für Bugs und Schwachstellen} und \textbf{81,2\% für Code Smells} mit kombinierten GPT-Modellen erreicht wurden.
    \end{itemize}
    \item \textbf{Präzision und Recall bei der Schwachstellen-Erkennung:}
    \begin{itemize}
        \item LLift zeigte bei der Erkennung von Use-Before-Initialization (UBI)-Bugs eine \textbf{Präzision von 50\% und einen Recall von 100\%}, was bedeutet, dass keine echten Bugs übersehen wurden.
        \item LLMs übertreffen traditionelle Tools wie CodeQL bei bestimmten Schwachstellen-Klassen wie OS Command Injection und NULL Pointer Dereference.
    \end{itemize}
    \item \textbf{Reduzierung von False Positives (FPs) in SAST-Berichten (Konservative Analyse):}
    \begin{itemize}
        \item Das zentrale Ziel ist, FPs zu eliminieren, während ein \textbf{perfekter True Positive Rate (TPR) von 100\%} beibehalten wird, um keine echten Schwachstellen zu übersehen.
        \item In Benchmark-Datensätzen konnten \textbf{bis zu 62,5\% der FPs durch einzelne LLMs} und \textbf{bis zu 78,9\% durch die Kombination mehrerer LLMs} erkannt und gefiltert werden, ohne True Positives zu beeinträchtigen.
        \item In realen Szenarien wurden \textbf{33,85\% der FPs durch das beste LLM} und \textbf{38,46\% durch kombinierte Bewertungen} gefiltert.
    \end{itemize}
    \item \textbf{Kosteneffizienz:}
    \begin{itemize}
        \item Reduzierung der Kosten für Code-Revisionen: Ein Beispiel zeigt, dass die Revision von über 7.500 Problemen weniger als drei Stunden dauerte und weniger als 35 USD kostete.
    \end{itemize}
\end{itemize}

\subsubsection*{Umsetzung: Wie weit wurde diese Vision bereits materialisiert und was ist die Bedeutung für die Praxis? Was sind Aspekte, die bereits umgesetzt sind und wie?}

Die Vision der LLM-gestützten Softwareanalyse ist bereits in verschiedenen Prototypen und Frameworks materialisiert, was eine erhebliche Bedeutung für die praktische Softwareentwicklung und Cybersicherheit hat.

\textbf{Bereits umgesetzte Aspekte und deren Funktionsweise:}

\begin{enumerate}
    \item \textbf{Integrierte Pipelines zur Code-Revision:}
    \begin{itemize}
        \item \textbf{Static Code Analysis (SCA):} Die Prozesse beginnen mit einem \textbf{statischen Code-Analyse-Framework} (z.B. \textbf{SonarQube}), das Code-Dateien umfassend scannt und Probleme wie Bugs, Schwachstellen und Code Smells identifiziert.
        \item \textbf{Datenextraktion und -organisation:} Detaillierte Informationen zu jedem erkannten Problem (Dateiname, Zeilennummer, Problembeschreibung, vorgeschlagene Lösungen) werden extrahiert und strukturiert aufbereitet, um die Interaktion mit den LLMs zu erleichtern.
        \item \textbf{LLM-Integration:} Die aufbereiteten Problembeschreibungen und der betroffene Code werden als Eingabe (Prompt) an Large Language Models (LLMs) gesendet. Dabei werden in der Regel \textbf{kostengünstigere Modelle (z.B. OpenAI’s GPT-3.5 Turbo)} für eine erste Revision eingesetzt und \textbf{leistungsstärkere Modelle (z.B. GPT-4o)} für verbleibende oder komplexere Probleme genutzt.
    \end{itemize}

    \item \textbf{Spezialisierung und Optimierung von LLMs:}
    \begin{itemize}
        \item \textbf{Prompt Engineering:} Dies ist ein zentraler Aspekt und ein \textbf{iterativer Prozess} (oft über 20 Iterationen), um Prompts zu entwerfen und zu verfeinern. Dazu gehören:
        \begin{itemize}
            \item \textbf{Kontextualisierung:} Angabe der Programmiersprache und detaillierte Aufgabenstellungen basierend auf den extrahierten Problemdetails.
            \item \textbf{Few-shot Learning:} Beispiele für korrekte Korrekturen pro Problemtyp werden in den Prompt aufgenommen, um dem LLM die gewünschte Struktur und Logik beizubringen.
            \item \textbf{Chain-of-Thought (CoT) Reasoning:} LLMs werden angewiesen, ihre Überlegungen schrittweise darzulegen, was die Qualität der Antworten verbessert und die Nachvollziehbarkeit erhöht.
            \item \textbf{Task Decomposition:} Komplexe Aufgaben werden in kleinere, überschaubare Unteraufgaben unterteilt, die sequenziell von den LLMs bearbeitet werden, was die Präzision und Konsistenz der Ergebnisse verbessert.
            \item \textbf{Progressive Prompts:} LLMs sind so konfiguriert, dass sie bei Unsicherheiten oder fehlenden Funktionsdefinitionen aktiv nach zusätzlichen Informationen fragen können, die dann dynamisch aus dem Quellcode bereitgestellt werden.
            \item \textbf{Self-Validation:} LLMs werden dazu angeleitet, ihre eigenen generierten Antworten zu überprüfen und zu korrigieren, um Inkonsistenzen und Halluzinationen zu minimieren.
            \item \textbf{Persona-Zuweisung:} Den LLMs wird eine Rolle (z.B. „Sicherheitsexperte“) zugewiesen, um die Relevanz und den Fokus der generierten Antworten zu optimieren.
        \end{itemize}
        \item \textbf{Retrieval-Augmented Generation (RAG):} Diese Technik ermöglicht den LLMs den Zugriff auf \textbf{aktuelle externe Informationen} vor der Generierung der endgültigen Ausgabe.
        \begin{itemize}
            \item \textbf{Dynamische Wissensbasis:} Relevante Dokumente und Lösungen werden aus externen Quellen wie \textbf{Stack Overflow, GitHub, SonarQube Community und Google Search} abgerufen. Neuere Ansätze sammeln dafür auch aktuelle Schwachstellenberichte von Plattformen wie HackerOne.
            \item \textbf{Ähnlichkeitssuche:} Statt alle verfügbaren Informationen zu übermitteln, wird eine Ähnlichkeitssuche durchgeführt, um die relevantesten Berichte basierend auf der \textbf{Code-Funktionalität} oder der \textbf{Code-Abstraktion (minimierte Code-Repräsentation)} zu finden. Die Abstraktionsmethode hat sich als effektiver erwiesen, da sie enger mit Schwachstellen-Mustern korreliert.
        \end{itemize}
    \end{itemize}

    \item \textbf{Verifizierung und Fehlerkorrektur:}
    \begin{itemize}
        \item \textbf{Code Comparison App:} Eine eigens entwickelte Anwendung ermöglicht den Seiten-an-Seiten-Vergleich des revidierten Codes mit dem Original, um Änderungen hervorzuheben und \textbf{Halluzinationen des LLMs (plausible, aber inkorrekte Ausgaben)} zu erkennen.
        \item \textbf{Menschliche Intervention:} Bei erkannten Halluzinationen erfolgt ein manueller Eingriff zur Überprüfung und Korrektur, bevor der Code final in die Codebasis übernommen wird.
    \end{itemize}
\end{enumerate}

\textbf{Bedeutung für die Praxis und Anwendungsbeispiele:}

\begin{itemize}
    \item \textbf{LLift-Framework:} Ein konkretes Beispiel ist LLift, das statische Analyse mit LLMs verbindet, um \textbf{Use Before Initialization (UBI)-Bugs im Linux-Kernel} zu finden. LLift konnte \textbf{13 bisher unentdeckte UBI-Bugs identifizieren, von denen vier von der Linux-Community als echte Bugs bestätigt wurden}. Es konnte empirisch alle bekannten UBI-Bugs, die von einem traditionellen Tool (UBITect) erkannt wurden, ebenfalls identifizieren.
    \item \textbf{LSAST-Framework:} LSAST ist ein Ansatz zur Verbesserung der Cybersicherheit, der LLMs mit SAST-Scannern (wie \textbf{Bearer}) integriert, um die Erkennung komplexer Schwachstellen zu verbessern und False Positives zu reduzieren. In einem realen Python-Projekt (pyDust) identifizierte LSAST \textbf{6 Schwachstellen (darunter 2 High-Risk), die von konventionellen Scannern übersehen wurden}, was die praktische Anwendbarkeit in realen Umgebungen unterstreicht.
    \item \textbf{Reduzierung manueller Überprüfungen:} Die LLM-basierte Bewertung von SAST-Ergebnissen kann einen \textbf{"konservativen" Analyseansatz} verfolgen, bei dem FPs gefiltert werden, ohne True Positives zu übersehen. Dies bedeutet eine \textbf{signifikante Reduzierung des Ressourcenaufwands für manuelle Sicherheitsüberprüfungen} durch Sicherheitsexperten.
    \item \textbf{Verbesserte Code-Verständlichkeit und Feedback:} LLMs können nicht nur Korrekturen liefern, sondern auch Erklärungen in natürlicher Sprache für ihre Vorschläge, was Entwicklern hilft, die zugrunde liegenden Probleme besser zu verstehen und zu lernen.
\end{itemize}

\subsubsection*{Zukünftige Themen: Was sind aktuelle Schwächen dieses Ansatzes/Verfahrens? Was sind zukünftige Forschungsthemen (offene Forschungsfragen)?}

Obwohl die Integration von LLMs in die Softwareanalyse vielversprechend ist, gibt es noch erhebliche Schwächen und eine Reihe offener Forschungsfragen, die zukünftige Arbeiten adressieren müssen.

\textbf{Aktuelle Schwächen und Einschränkungen des Ansatzes/Verfahrens:}

\begin{itemize}
    \item \textbf{LLM-spezifische Herausforderungen:}
    \begin{itemize}
        \item \textbf{Halluzinationen und Inkonsistenz:} LLMs können weiterhin plausible, aber inkorrekte oder nicht fundierte Ausgaben generieren, die neue Probleme in den Code einführen oder ungenaue Korrekturen liefern können. Trotz Maßnahmen wie "Code Comparison Apps" und Selbstvalidierung bleibt die vollständige Eliminierung dieser Halluzinationen eine Herausforderung.
        \item \textbf{Stochastizität:} Die inhärente Unvorhersehbarkeit der LLM-Outputs kann zu inkonsistenten Analysen führen, selbst bei niedrigen Temperatur-Einstellungen, die die Explorationsfähigkeit des Modells beeinträchtigen können.
        \item \textbf{Token-Begrenzung:} Die begrenzte Kontextfenstergröße von LLMs stellt eine fundamentale Herausforderung für die Analyse großer oder komplexer Codebasen dar, da nicht alle relevanten Informationen gleichzeitig im Prompt enthalten sein können.
        \item \textbf{Begrenztes Verständnis von Post-Constraints und domänenspezifischem Wissen:} Aktuelle LLMs sind nicht nativ sensibel für alle Arten von Post-Constraints und können diese übersehen oder falsch interpretieren, was zu inkorrekten Bug-Identifikationen führen kann.
        \item \textbf{Generalisierbarkeit:} Die aktuellen Erkenntnisse und die Effektivität des Ansatzes sind möglicherweise nicht auf alle Programmiersprachen, Codebasen oder Schwachstellen-Kategorien übertragbar. Zudem kann es schwierig sein, konservative Analysen auf ungelabelten Daten zu garantieren.
        \item \textbf{Skalierung und Leistung:} Einige Studien haben gezeigt, dass die reine Erhöhung der Modellgröße nicht immer zu einer besseren Schwachstellen-Erkennung führt.
    \end{itemize}
    \item \textbf{Integrations- und Analyseprobleme:}
    \begin{itemize}
        \item \textbf{Informationslücken:} Die Dekopplung von LLM und statischer Analyse kann zu Informationslücken führen (z.B. fehlende explizite Feldnamen innerhalb einer Struktur), was die Präzision der LLM-Analyse beeinträchtigt.
        \item \textbf{Umgang mit komplexen Code-Mustern:} Schwierigkeiten bei der Analyse von Schleifen, Nebenläufigkeit und indirekten Aufrufen (Callback-Funktionen, Funktionszeiger) bestehen weiterhin, auch wenn LLMs hierbei Unterstützung bieten können.
        \item \textbf{Variablen mit gleichem Namen:} LLMs können dazu neigen, verschiedene Variablen in unterschiedlichen Scopes zu verwechseln, wenn sie den gleichen Namen tragen.
        \item \textbf{Abhängigkeit von proprietären LLMs:} Die Nutzung von Closed-Source-APIs wie GPT-4, die häufig aktualisiert werden, schränkt die Reproduzierbarkeit und externe Validität der Forschung ein.
    \end{itemize}
    \item \textbf{Evaluierungsherausforderungen:}
    \begin{itemize}
        \item \textbf{Fehlen automatisierter Testfälle:} Für die automatische Verifizierung LLM-generierter Code-Dateien fehlen oft umfassende Testfälle. Dies erfordert derzeit noch manuelle Verifizierungsschritte.
        \item \textbf{Datenverzerrung:} Die Auswahl der zugrunde liegenden statischen Analyse-Tools oder Trainings-Datensätze kann zu Verzerrungen in den Ergebnissen führen.
    \end{itemize}
\end{itemize}

\textbf{Zukünftige Forschungsthemen (offene Forschungsfragen):}

\begin{itemize}
    \item \textbf{Erkundung und Vergleich weiterer LLM-Modelle:} Umfassende vergleichende Analysen mit einer breiteren Palette von LLMs, einschließlich Google Gemini, Anthropic Claude und verschiedenen Versionen der Open-Source LLaMA-Modelle, um die effektivsten Optionen zu identifizieren.
    \item \textbf{Verfeinerung von Retrieval-Augmented Generation (RAG)-Techniken:}
    \begin{itemize}
        \item \textbf{Optimierung der Relevanz:} Verbesserung der Mechanismen zur Filterung und Rangordnung abgerufener Informationen, um sicherzustellen, dass nur die relevantesten Daten an das LLM übermittelt werden, da irrelevante Informationen die Leistung beeinträchtigen können.
        \item \textbf{Adaptive Lernmechanismen:} Entwicklung von Systemen, die es LLMs ermöglichen, ihre Wissensbasis kontinuierlich mit den neuesten Sicherheitsbedrohungen und Gegenmaßnahmen zu aktualisieren.
    \end{itemize}
    \item \textbf{Fortgeschrittenes Prompt Engineering:} Weiterentwicklung von Prompt-Techniken, einschließlich Rollenspiel und noch präziser geführter Antworten, um spezifische Probleme und Kontexte besser zu adressieren. Auch aufgabenspezifische Prompts für Schwachstellenkategorien mit hohen Fehlerraten könnten erforscht werden.
    \item \textbf{Tiefergehende Integration von LLMs mit statischer Analyse:}
    \begin{itemize}
        \item \textbf{Neuro-symbolische Ansätze:} Entwicklung von Techniken, die die intuitive Denkfähigkeit von LLMs mit der formalen Präzision symbolischer Tools (wie Logik-Engines und statischen Code-Analysatoren) kombinieren, um effektivere und interpretierbarere Lösungen zu schaffen.
        \item \textbf{LLM als "Assistent":} Selektive Nutzung von LLMs als intelligente Assistenten, um spezifische Hürden der statischen Analyse zu überwinden (z.B. Skalierungsprobleme, Zusammenfassen von Schleifeninvarianten, Behandlung indirekter Aufrufe).
        \item \textbf{Verbesserung der Datenflüsse:} Engere Integration der Informationen aus statischer Analyse in die LLM-Prompts, um Informationslücken zu schließen und die Präzision zu erhöhen.
    \end{itemize}
    \item \textbf{Automatisierung der Verifizierung und Testfall-Generierung:}
    \begin{itemize}
        \item Entwicklung von Methoden, bei denen LLMs initial Testfälle für Originaldateien generieren, die dann zur Verifizierung der LLM-generierten Code-Revisionen verwendet werden. Dies könnte manuelle Verifizierungsschritte minimieren.
        \item Kombination von LLM-generierten Revisionen mit automatisierten Test-Frameworks, um die Code-Zuverlässigkeit zu erhöhen und neue Probleme zu verhindern.
    \end{itemize}
    \item \textbf{Generalisierung auf weitere Bug-Typen und Domänen:} Erweiterung der Methoden über UBI-Bugs hinaus auf andere komplexe Schwachstellen wie Use-After-Free, Out-of-Bounds Read/Write und Taint Flow Analysis. Auch die Anwendung auf andere Programmiersprachen und diverse Projekte ist ein wichtiger Schritt.
    \item \textbf{Integration in CI/CD-Pipelines:} Nahtlose Integration von LLM-basierten Systemen in Continuous Integration / Continuous Deployment (CI/CD)-Pipelines, um Echtzeit-Feedback zu liefern und automatisierte Korrekturen während des gesamten Entwicklungszyklus zu ermöglichen.
    \item \textbf{Ensemble-Methoden:} Weiterentwicklung von Strategien zur Kombination der Vorhersagen mehrerer LLMs, um die Erkennungsrate von False Positives weiter zu steigern, während die Konservativität gewahrt bleibt.
    \item \textbf{Feinabstimmung von LLMs:} Evaluierung, ob speziell feinabgestimmte, domänenspezifische LLMs General-Purpose-Modelle in bestimmten Anwendungsfällen übertreffen können.
\end{itemize}
```