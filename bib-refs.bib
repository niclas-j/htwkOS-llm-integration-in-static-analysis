@article{liEnhancingStaticAnalysis2024,
  title = {Enhancing {{Static Analysis}} for {{Practical Bug Detection}}: {{An LLM-Integrated Approach}}},
  shorttitle = {Enhancing {{Static Analysis}} for {{Practical Bug Detection}}},
  author = {Li, Haonan and Hao, Yu and Zhai, Yizhuo and Qian, Zhiyun},
  year = {2024},
  month = apr,
  journal = {Enhancing Static Analysis for Practical Bug Detection: An LLM-Integrated Approach (Artifact)},
  volume = {8},
  number = {OOPSLA1},
  pages = {111:474--111:499},
  doi = {10.1145/3649828},
  urldate = {2025-06-12},
  abstract = {While static analysis is instrumental in uncovering software bugs, its precision in analyzing large and intricate codebases remains challenging. The emerging prowess of Large Language Models (LLMs) offers a promising avenue to address these complexities. In this paper, we present LLift, a pioneering framework that synergizes static analysis and LLMs, with a spotlight on identifying use-before-initialization (UBI) bugs within the Linux kernel. Drawing from our insights into variable usage conventions in Linux, we enhance path analysis using post-constraint guidance. This approach, combined with our methodically crafted procedures, empowers LLift to adeptly handle the challenges of bug-specific modeling, extensive codebases, and the unpredictable nature of LLMs. Our real-world evaluations identified four previously undiscovered UBI bugs in the mainstream Linux kernel, which the Linux community has acknowledged. This study reaffirms the potential of marrying static analysis with LLMs, setting a compelling direction for future research in this area.},
  file = {/Users/niclas/Zotero/storage/JB6RDE4X/Li et al. - 2024 - Enhancing Static Analysis for Practical Bug Detection An LLM-Integrated Approach.pdf}
}

@article{wagnerEffectiveComplementarySecurity2025,
  title = {Towards {{Effective Complementary Security Analysis}} Using {{Large Language Models}}},
  author = {Wagner, Jonas and Müller, Simon and Näther, Christian and Steghöfer, Jan-Philipp and Both, Andreas},
  date = {2025-06-20},
  eprint = {2506.16899},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2506.16899},
  url = {http://arxiv.org/abs/2506.16899},
  urldate = {2025-06-27},
  abstract = {A key challenge in security analysis is the manual evaluation of potential security weaknesses generated by static application security testing (SAST) tools. Numerous false positives (FPs) in these reports reduce the effectiveness of security analysis. We propose using Large Language Models (LLMs) to improve the assessment of SAST findings. We investigate the ability of LLMs to reduce FPs while trying to maintain a perfect true positive rate, using datasets extracted from the OWASP Benchmark (v1.2) and a real-world software project. Our results indicate that advanced prompting techniques, such as Chain-of-Thought and Self-Consistency, substantially improve FP detection. Notably, some LLMs identified approximately 62.5\% of FPs in the OWASP Benchmark dataset without missing genuine weaknesses. Combining detections from different LLMs would increase this FP detection to approximately 78.9\%. Additionally, we demonstrate our approach's generalizability using a real-world dataset covering five SAST tools, three programming languages, and infrastructure files. The best LLM detected 33.85\% of all FPs without missing genuine weaknesses, while combining detections from different LLMs would increase this detection to 38.46\%. Our findings highlight the potential of LLMs to complement traditional SAST tools, enhancing automation and reducing resources spent addressing false alarms.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Cryptography and Security},
  file = {/Users/niclas/Zotero/storage/FVC7UMAR/Wagner et al. - 2025 - Towards Effective Complementary Security Analysis using Large Language Models.pdf;/Users/niclas/Zotero/storage/PPKRL7ER/2506.html}
}

@article{mollerDepartmentComputerScience,
  title = {Department of {{Computer Science Aarhus University}}, {{Denmark}}},
  author = {Møller, Anders and Schwartzbach, Michael I},
  langid = {english},
  year = {2025},
  journal = {Department of Computer Science, Aarhus University},
  file = {/Users/niclas/Zotero/storage/9LSUK78T/Møller and Schwartzbach - Department of Computer Science Aarhus University, Denmark.pdf}
}

@inproceedings{liIRISLLMAssistedStatic2024,
  title = {{{IRIS}}: {{LLM-Assisted Static Analysis}} for {{Detecting Security Vulnerabilities}}},
  shorttitle = {{{IRIS}}},
  author = {Li, Ziyang and Dutta, Saikat and Naik, Mayur},
  date = {2024-10-04},
  url = {https://openreview.net/forum?id=9LdJDU7E91},
  urldate = {2025-06-24},
  abstract = {Software is prone to security vulnerabilities. Program analysis tools to detect them have limited effectiveness in practice due to their reliance on human labeled specifications. Large language models (or LLMs) have shown impressive code generation capabilities but they cannot do complex reasoning over code to detect such vulnerabilities especially since this task requires whole-repository analysis. We propose IRIS, a neuro-symbolic approach that systematically combines LLMs with static analysis to perform whole-repository reasoning for security vulnerability detection. Specifically, IRIS leverages LLMs to infer taint specifications and perform contextual analysis, alleviating needs for human specifications and inspection. For evaluation, we curate a new dataset, CWE-Bench-Java, comprising 120 manually validated security vulnerabilities in real-world Java projects. A state-of-the-art static analysis tool CodeQL detects only 27 of these vulnerabilities whereas IRIS with GPT-4 detects 55 (+28) and improves upon CodeQL's average false discovery rate by 5\% points. Furthermore, IRIS identifies 4 previously unknown vulnerabilities which cannot be found by existing tools. IRIS is available publicly at https://github.com/iris-sast/iris.},
  eventtitle = {The {{Thirteenth International Conference}} on {{Learning Representations}}},
  langid = {english},
  file = {/Users/niclas/Zotero/storage/WFVM34S8/Li et al. - 2024 - IRIS LLM-Assisted Static Analysis for Detecting Security Vulnerabilities.pdf}
}

@inproceedings{khareUnderstandingEffectivenessLarge2024,
    booktitle = {},
  title = {Understanding the {{Effectiveness}} of {{Large Language Models}} in {{Detecting Security Vulnerabilities}}},
  author = {Khare, Avishree and Dutta, Saikat and Li, Ziyang and Solko-Breslin, Alaia and Alur, Rajeev and Naik, Mayur},
  date = {2024-10-23},
  eprint = {2311.16169},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2311.16169},
  url = {http://arxiv.org/abs/2311.16169},
  urldate = {2025-06-19},
  abstract = {While automated vulnerability detection techniques have made promising progress in detecting security vulnerabilities, their scalability and applicability remain challenging. The remarkable performance of Large Language Models (LLMs), such as GPT-4 and CodeLlama, on code-related tasks has prompted recent works to explore if LLMs can be used to detect vulnerabilities. In this paper, we perform a more comprehensive study by concurrently examining a higher number of datasets, languages and LLMs, and qualitatively evaluating performance across prompts and vulnerability classes while addressing the shortcomings of existing tools. Concretely, we evaluate the effectiveness of 16 pre-trained LLMs on 5,000 code samples from five diverse security datasets. These balanced datasets encompass both synthetic and real-world projects in Java and C/C++ and cover 25 distinct vulnerability classes. Overall, LLMs across all scales and families show modest effectiveness in detecting vulnerabilities, obtaining an average accuracy of 62.8\% and F1 score of 0.71 across datasets. They are significantly better at detecting vulnerabilities only requiring intra-procedural analysis, such as OS Command Injection and NULL Pointer Dereference. Moreover, they report higher accuracies on these vulnerabilities than popular static analysis tools, such as CodeQL. We find that advanced prompting strategies that involve step-by-step analysis significantly improve performance of LLMs on real-world datasets in terms of F1 score (by upto 0.18 on average). Interestingly, we observe that LLMs show promising abilities at performing parts of the analysis correctly, such as identifying vulnerability-related specifications and leveraging natural language information to understand code behavior (e.g., to check if code is sanitized). We expect our insights to guide future work on LLM-augmented vulnerability detection systems.},
  pubstate = {prepublished},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Programming Languages,Computer Science - Software Engineering},
  file = {/Users/niclas/Zotero/storage/9DIJ2I9P/Khare et al. - 2024 - Understanding the Effectiveness of Large Language Models in Detecting Security Vulnerabilities.pdf;/Users/niclas/Zotero/storage/MTTWLYLT/2311.html}
}

@inproceedings{xiaAutomatedProgramRepair2023,
  title = {Automated {{Program Repair}} in the {{Era}} of {{Large Pre-trained Language Models}}},
  booktitle = {2023 {{IEEE}}/{{ACM}} 45th {{International Conference}} on {{Software Engineering}} ({{ICSE}})},
  author = {Xia, Chunqiu Steven and Wei, Yuxiang and Zhang, Lingming},
  date = {2023-05},
  pages = {1482--1494},
  issn = {1558-1225},
  doi = {10.1109/ICSE48619.2023.00129},
  url = {https://ieeexplore.ieee.org/document/10172803},
  urldate = {2025-06-29},
  abstract = {Automated Program Repair (APR) aims to help developers automatically patch software bugs. However, current state-of-the-art traditional and learning-based APR techniques face the problem of limited patch variety, failing to fix complicated bugs. This is mainly due to the reliance on bug-fixing datasets to craft fix templates (traditional) or directly predict potential patches (learning-based). Large Pre-Trained Language Models (LLMs), trained using billions of text/code tokens, can potentially help avoid this issue. Very recently, researchers have directly leveraged LLMs for APR without relying on any bug-fixing datasets. Meanwhile, such existing work either failed to include state-of-the-art LLMs or was not evaluated on realistic datasets. Thus, the true power of modern LLMs on the important APR problem is yet to be revealed. In this work, we perform the first extensive study on directly applying LLMs for APR. We select 9 recent state-of-the-art LLMs, including both generative and infilling models, ranging from 125M to 20B in size. We designed 3 different repair settings to evaluate the different ways we can use LLMs to generate patches: 1) generate the entire patch function, 2) fill in a chunk of code given the prefix and suffix 3) output a single line fix. We apply the LLMs under these repair settings on 5 datasets across 3 different languages and compare different LLMs in the number of bugs fixed, generation speed and compilation rate. We also compare the LLMs against recent state-of-the-art APR tools. Our study demonstrates that directly applying state-of-the-art LLMs can already substantially outperform all existing APR techniques on all our datasets. Among the studied LLMs, the scaling effect exists for APR where larger models tend to achieve better performance. Also, we show for the first time that suffix code after the buggy line (adopted in infilling-style APR) is important in not only generating more fixes but more patches with higher compilation rate. Besides patch generation, the LLMs consider correct patches to be more natural than other ones, and can even be leveraged for effective patch ranking or patch correctness checking. Lastly, we show that LLM-based APR can be further substantially boosted via: 1) increasing the sample size, and 2) incorporating fix template information.},
  eventtitle = {2023 {{IEEE}}/{{ACM}} 45th {{International Conference}} on {{Software Engineering}} ({{ICSE}})},
  keywords = {Automated Program Repair,Codes,Computer bugs,Distance measurement,Faces,Machine Learning,Maintenance engineering,Software,Task analysis},
  file = {/Users/niclas/Zotero/storage/ML6ZSGJN/10172803.html}
}

@inproceedings{lemieuxCodaMosaEscapingCoverage2023,
  title = {{{CodaMosa}}: {{Escaping Coverage Plateaus}} in {{Test Generation}} with {{Pre-trained Large Language Models}}},
  shorttitle = {{{CodaMosa}}},
  booktitle = {2023 {{IEEE}}/{{ACM}} 45th {{International Conference}} on {{Software Engineering}} ({{ICSE}})},
  author = {Lemieux, Caroline and Inala, Jeevana Priya and Lahiri, Shuvendu K. and Sen, Siddhartha},
  date = {2023-05},
  pages = {919--931},
  publisher = {IEEE},
  location = {Melbourne, Australia},
  doi = {10.1109/ICSE48619.2023.00085},
  url = {https://ieeexplore.ieee.org/document/10172800/},
  urldate = {2025-06-29},
  abstract = {Search-based software testing (SBST) generates high-coverage test cases for programs under test with a combination of test case generation and mutation. SBST’s performance relies on there being a reasonable probability of generating test cases that exercise the core logic of the program under test. Given such test cases, SBST can then explore the space around them to exercise various parts of the program. This paper explores whether Large Language Models (LLMs) of code, such as OpenAI’s Codex, can be used to help SBST’s exploration. Our proposed algorithm, CODAMOSA, conducts SBST until its coverage improvements stall, then asks Codex to provide example test cases for under-covered functions. These examples help SBST redirect its search to more useful areas of the search space. On an evaluation over 486 benchmarks, CODAMOSA achieves statistically significantly higher coverage on many more benchmarks (173 and 279) than it reduces coverage on (10 and 4), compared to SBST and LLM-only baselines.},
  eventtitle = {2023 {{IEEE}}/{{ACM}} 45th {{International Conference}} on {{Software Engineering}} ({{ICSE}})},
  isbn = {978-1-6654-5701-9},
  langid = {english},
  file = {/Users/niclas/Zotero/storage/6E9KKC4R/Lemieux et al. - 2023 - CodaMosa Escaping Coverage Plateaus in Test Generation with Pre-trained Large Language Models.pdf}
}

@inproceedings{chapmanInterleavingStaticAnalysis2024,
  title = {Interleaving {{Static Analysis}} and {{LLM Prompting}}},
  booktitle = {Proceedings of the 13th {{ACM SIGPLAN International Workshop}} on the {{State Of}} the {{Art}} in {{Program Analysis}}},
  author = {Chapman, Patrick J. and Rubio-González, Cindy and Thakur, Aditya V.},
  date = {2024-06-20},
  series = {{{SOAP}} 2024},
  pages = {9--17},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3652588.3663317},
  url = {https://dl.acm.org/doi/10.1145/3652588.3663317},
  urldate = {2025-06-17},
  abstract = {This paper presents a new approach for using Large Language Models (LLMs) to improve static program analysis. Specifically, during program analysis, we interleave calls to the static analyzer  and queries to the LLM: the prompt used to query the LLM is constructed using intermediate results from the static analysis, and the result from the LLM query is used for subsequent analysis of the program. We apply this novel approach to the problem of error-specification inference of functions in systems code written in C; i.e., inferring the set of values returned by each function upon error, which can aid in program understanding as well as in finding error-handling bugs. We evaluate our approach on real-world C programs, such as MbedTLS and zlib, by incorporating LLMs into EESI, a state-of-the-art static analysis for error-specification inference. Compared to EESI, our approach achieves higher recall across all benchmarks (from average of 52.55\% to 77.83\%) and higher F1-score (from average of 0.612 to 0.804) while maintaining precision (from average of 86.67\% to 85.12\%).},
  isbn = {979-8-4007-0621-9},
  file = {/Users/niclas/Zotero/storage/QUMP68IH/Chapman et al. - 2024 - Interleaving Static Analysis and LLM Prompting.pdf}
}
